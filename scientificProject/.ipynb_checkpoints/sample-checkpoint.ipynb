{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "To debug a cell, press Alt+Shift+Enter, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install seaborn\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc27f8-eebe-43d1-8ad0-e007865bf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_car_age(df):\n",
    "    \"\"\"\n",
    "    Add a column calculating the difference between posting year and car year.\n",
    "    Uses the posting_year column that was already created by clean_car_data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'year' and 'posting_year' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original dataframe with new 'car_age' column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate the age of the car at posting time\n",
    "    df['car_age'] = df['posting_year'] - df['year']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_and_clean_single_vehicle_file(file_path):\n",
    "    \"\"\"\n",
    "    Load, clean, normalize a vehicle data file, calculate car age, \n",
    "    and save the cleaned DataFrame with a name including the earliest \n",
    "    and latest posting dates.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the vehicle data CSV file to process\n",
    "    \n",
    "    Returns:\n",
    "    None: The cleaned DataFrame is written to a new CSV file\n",
    "    \"\"\"\n",
    "    print(\"Starting to process and clean the vehicle file...\")\n",
    "\n",
    "    # Extract the file name and determine posting_date if applicable\n",
    "    file_name = os.path.basename(file_path)\n",
    "    parts = file_name.replace('.csv', '').split('-')\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        year = parts[-1]\n",
    "        month = parts[-2]\n",
    "        default_posting_date = f\"{year}-{month}-01\"\n",
    "    else:\n",
    "        default_posting_date = None  # No default posting_date if the file doesn't follow the pattern\n",
    "    \n",
    "    # Load CSV into a DataFrame\n",
    "    print(f\"Reading file: {file_name}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Add posting_date column if it doesn't exist\n",
    "    if 'posting_date' not in df.columns:\n",
    "        df['posting_date'] = default_posting_date\n",
    "    \n",
    "    # Clean and normalize the DataFrame\n",
    "    print(\"Cleaning and normalizing data...\")\n",
    "    cleaned_df = clean_car_data(df)\n",
    "    \n",
    "    # Add car age\n",
    "    print(\"Adding car age column...\")\n",
    "    cleaned_df = add_car_age(cleaned_df)\n",
    "    \n",
    "    # Convert posting_date to datetime for sorting and range determination\n",
    "    cleaned_df['posting_date'] = pd.to_datetime(cleaned_df['posting_date'], errors='coerce')\n",
    "    earliest_date = cleaned_df['posting_date'].min().strftime('%Y-%m-%d') if not cleaned_df['posting_date'].isna().all() else 'unknown'\n",
    "    latest_date = cleaned_df['posting_date'].max().strftime('%Y-%m-%d') if not cleaned_df['posting_date'].isna().all() else 'unknown'\n",
    "    \n",
    "    # Construct the output file name\n",
    "    output_file_name = f\"cleaned-vehicles-{earliest_date}-{latest_date}.csv\"\n",
    "    output_path = os.path.join(os.path.dirname(file_path), output_file_name)\n",
    "    \n",
    "    # Write the cleaned DataFrame to the output file\n",
    "    print(f\"Writing cleaned data to: {output_file_name}\")\n",
    "    cleaned_df.to_csv(output_path, index=False)\n",
    "    print(\"Processing and cleaning complete.\")\n",
    "\n",
    "def analyze_and_normalize_data(df):\n",
    "    \"\"\"\n",
    "    Perform analysis and normalization of vehicle prices based on state-level factors.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Cleaned vehicle DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with normalized prices\n",
    "    \"\"\"\n",
    "    print(\"Calculating state price factors...\")\n",
    "    state_factors = calculate_state_price_factors(df)\n",
    "    print(\"State price factors calculated.\")\n",
    "    \n",
    "    print(\"Adding normalized prices...\")\n",
    "    df_with_normalized = add_normalized_prices(df)\n",
    "    print(\"Normalized prices added successfully.\")\n",
    "    \n",
    "    return df_with_normalized\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def clean_car_data(df):\n",
    "    \"\"\"\n",
    "    Clean car listing data by:\n",
    "    1. Removing rows with more than 2 missing values (ignoring specified columns)\n",
    "    2. Filling single missing values with medians from similar cars\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Raw car listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Cleaned dataset\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Extract year from posting_date string\n",
    "    cleaned_df['posting_year'] = cleaned_df['posting_date'].str[:4].astype(float)\n",
    "    \n",
    "    # Columns to ignore in missing value calculations\n",
    "    ignore_columns = ['long', 'lat', 'image_url', 'region', 'region_url', 'county', 'state']\n",
    "    \n",
    "    # Get columns to check for missing values (excluding ignored ones)\n",
    "    columns_to_check = [col for col in cleaned_df.columns if col not in ignore_columns]\n",
    "    \n",
    "    # Remove rows with more than 2 missing values in the relevant columns\n",
    "    rows_to_keep = cleaned_df[columns_to_check].isnull().sum(axis=1) <= 2\n",
    "    cleaned_df = cleaned_df[rows_to_keep]\n",
    "    \n",
    "    # Function to fill missing values based on similar cars\n",
    "    def fill_missing_value(row, column):\n",
    "        if pd.isnull(row[column]):\n",
    "            # Get median value for same make, model, year posted in same year\n",
    "            similar_cars = cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (cleaned_df['manufacturer'] == row['manufacturer']) &\n",
    "                (cleaned_df['model'] == row['model']) &\n",
    "                (cleaned_df['posting_year'] == row['posting_year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ]\n",
    "            \n",
    "            if len(similar_cars) >= 3:  # If we have enough similar cars\n",
    "                return similar_cars[column].median()\n",
    "            \n",
    "            # If not enough similar cars, broaden criteria (ignore model)\n",
    "            similar_cars = cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (cleaned_df['manufacturer'] == row['manufacturer']) &\n",
    "                (cleaned_df['posting_year'] == row['posting_year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ]\n",
    "            \n",
    "            if len(similar_cars) >= 3:\n",
    "                return similar_cars[column].median()\n",
    "            \n",
    "            # If still not enough, use global median for that year\n",
    "            return cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ][column].median()\n",
    "    \n",
    "    # Fill single missing values for numeric columns\n",
    "    numeric_columns = ['odometer', 'price']  # Add other numeric columns as needed\n",
    "    for column in numeric_columns:\n",
    "        mask = cleaned_df[column].isnull()\n",
    "        cleaned_df.loc[mask, column] = cleaned_df[mask].apply(\n",
    "            lambda row: fill_missing_value(row, column), axis=1\n",
    "        )\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def process_all_vehicle_files(folder_path):\n",
    "    \"\"\"\n",
    "    Process and clean all vehicle files in a given folder.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing vehicle data CSV files\n",
    "    \n",
    "    Returns:\n",
    "    None: Processed files are saved with new names in the same folder\n",
    "    \"\"\"\n",
    "    print(f\"Processing all vehicle files in folder: {folder_path}\")\n",
    "    \n",
    "    # Get a list of all CSV files in the folder starting with 'vehicles'\n",
    "    file_paths = glob(os.path.join(folder_path, \"vehicles*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No files found in the folder matching the pattern 'vehicles*.csv'.\")\n",
    "        return\n",
    "    \n",
    "    for idx, file_path in enumerate(file_paths, 1):\n",
    "        print(f\"Processing file {idx}/{len(file_paths)}: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            process_and_clean_single_vehicle_file(file_path)\n",
    "            print(f\"File {idx} processed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(\"All files in the folder have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce08bba-9065-40d3-b94f-aa06201cd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_all_vehicle_files(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634db1c1-164d-42ce-8e5f-573e22b6459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(input_folder):\n",
    "    \"\"\"\n",
    "    Processes all cleaned CSV files in the input folder, calculates state price factors,\n",
    "    and outputs the aggregated result to 'state_price_factors.csv'.\n",
    "\n",
    "    Parameters:\n",
    "    input_folder (str): Path to the folder containing the cleaned CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Aggregated state price factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_state_price_factors(df):\n",
    "        \"\"\"\n",
    "        Calculate how much each state's prices differ from the national average,\n",
    "        with outlier removal and minimum listing requirements.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Cleaned car listing DataFrame\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: State price factors showing percentage difference from national average\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create copy for analysis\n",
    "        df_analysis = df.copy()\n",
    "        \n",
    "        # Remove national outliers first\n",
    "        Q1 = df_analysis['price'].quantile(0.25)\n",
    "        Q3 = df_analysis['price'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        price_lower_bound = Q1 - 1.5 * IQR\n",
    "        price_upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_analysis = df_analysis[\n",
    "            (df_analysis['price'] >= price_lower_bound) &\n",
    "            (df_analysis['price'] <= price_upper_bound)\n",
    "        ]\n",
    "        \n",
    "        # Calculate national average after removing outliers\n",
    "        national_avg = df_analysis['price'].mean()\n",
    "        \n",
    "        # Function to remove outliers for a specific state\n",
    "        def get_state_mean_without_outliers(state_data):\n",
    "            if len(state_data) < 100:  # Minimum listings requirement\n",
    "                return None\n",
    "                \n",
    "            # Remove state-level outliers\n",
    "            Q1 = state_data['price'].quantile(0.25)\n",
    "            Q3 = state_data['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            \n",
    "            clean_state_data = state_data[\n",
    "                (state_data['price'] >= lower) &\n",
    "                (state_data['price'] <= upper)\n",
    "            ]\n",
    "            \n",
    "            if len(clean_state_data) < 50:  # Minimum cleaned listings requirement\n",
    "                return None\n",
    "                \n",
    "            return clean_state_data['price'].mean()\n",
    "        \n",
    "        # Calculate state-level statistics\n",
    "        state_stats = []\n",
    "        \n",
    "        for state in df_analysis['state'].unique():\n",
    "            state_data = df_analysis[df_analysis['state'] == state]\n",
    "            state_mean = get_state_mean_without_outliers(state_data)\n",
    "            \n",
    "            if state_mean is not None:\n",
    "                price_factor = (state_mean / national_avg - 1) * 100\n",
    "                \n",
    "                # Skip states with unrealistic price factors\n",
    "                if abs(price_factor) <= 50:  # Maximum allowed deviation\n",
    "                    state_stats.append({\n",
    "                        'state': state,\n",
    "                        'avg_price': state_mean,\n",
    "                        'price_factor': price_factor,\n",
    "                        'listing_count': len(state_data)\n",
    "                    })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        state_stats = pd.DataFrame(state_stats)\n",
    "        \n",
    "        # Sort by price factor\n",
    "        state_stats = state_stats.sort_values('price_factor', ascending=False)\n",
    "        \n",
    "        # Round numbers for readability\n",
    "        state_stats['avg_price'] = state_stats['avg_price'].round(2)\n",
    "        state_stats['price_factor'] = state_stats['price_factor'].round(2)\n",
    "        \n",
    "        return state_stats\n",
    "\n",
    "    # Find all cleaned CSV files\n",
    "    csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                 if f.startswith('cleaned-') and f.endswith('.csv')]\n",
    "    \n",
    "    # Aggregate data from all files\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        print(f\"starting work on {file}\") \n",
    "        df = pd.read_csv(file)\n",
    "        aggregated_df = pd.concat([aggregated_df, df], ignore_index=True)\n",
    "    \n",
    "    # Calculate state price factors\n",
    "    state_price_factors = calculate_state_price_factors(aggregated_df)\n",
    "    \n",
    "    # Save the result to CSV\n",
    "    output_file = os.path.join(input_folder, 'state_price_factors.csv')\n",
    "    state_price_factors.to_csv(output_file, index=False)\n",
    "    \n",
    "    return state_price_factors\n",
    "\n",
    "def adjust_price_for_location(price, state, state_factors):\n",
    "    \"\"\"\n",
    "    Adjust a car's price to account for state-level price differences.\n",
    "    Returns original price if state adjustment isn't available.\n",
    "    \n",
    "    Parameters:\n",
    "    price (float): Original price\n",
    "    state (str): State where the car is listed\n",
    "    state_factors (pandas.DataFrame): DataFrame with state price factors\n",
    "    \n",
    "    Returns:\n",
    "    float: Nationally adjusted price\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the state's price factor\n",
    "        state_factor = state_factors.loc[state_factors['state'] == state, 'price_factor'].iloc[0]\n",
    "        \n",
    "        # Adjust price by removing state factor\n",
    "        adjusted_price = price / (1 + state_factor/100)\n",
    "        \n",
    "        return round(adjusted_price, 2)\n",
    "    except:\n",
    "        # Return original price if state adjustment isn't available\n",
    "        return price\n",
    "\n",
    "def add_normalized_prices(df):\n",
    "    \"\"\"\n",
    "    Add a normalized_price column to the DataFrame that adjusts for state-level price differences.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing car listings\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with new normalized_price column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_normalized = df.copy()\n",
    "    \n",
    "    # Calculate state factors\n",
    "    state_factors = calculate_state_price_factors(df)\n",
    "    \n",
    "    # Create normalized_price column using vectorized operations\n",
    "    df_with_normalized['state_price_factor'] = df_with_normalized['state'].map(\n",
    "        state_factors.set_index('state')['price_factor'].fillna(0)\n",
    "    )\n",
    "    \n",
    "    df_with_normalized['normalized_price'] = df_with_normalized.apply(\n",
    "        lambda row: adjust_price_for_location(row['price'], row['state'], state_factors),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_with_normalized\n",
    "\n",
    "\n",
    "def visualize_state_prices(filename='state_price_factors.csv'):\n",
    "    \"\"\"\n",
    "    Read and visualize state price factors data.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Path to the state price factors CSV file\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Sort by price factor\n",
    "        df = df.sort_values('price_factor')\n",
    "        \n",
    "        # Create figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[2, 1])\n",
    "        \n",
    "        # Bar plot\n",
    "        sns.barplot(data=df, x='state', y='price_factor', ax=ax1)\n",
    "        ax1.set_title('State Price Differences from National Average')\n",
    "        ax1.set_xlabel('State')\n",
    "        ax1.set_ylabel('Price Difference (%)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add horizontal line at 0\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        # Text summary\n",
    "        summary_text = (\n",
    "            f\"Summary Statistics:\\n\"\n",
    "            f\"Number of states: {len(df)}\\n\"\n",
    "            f\"Price factor range: {df['price_factor'].min():.1f}% to {df['price_factor'].max():.1f}%\\n\"\n",
    "            f\"Total listings analyzed: {df['listing_count'].sum():,}\\n\\n\"\n",
    "            f\"Top 3 most expensive states:\\n\"\n",
    "            + \"\\n\".join(f\"{state}: +{factor:.1f}%\" \n",
    "                       for state, factor in df.nlargest(3, 'price_factor')[['state', 'price_factor']].values)\n",
    "            + \"\\n\\nTop 3 least expensive states:\\n\"\n",
    "            + \"\\n\".join(f\"{state}: {factor:.1f}%\" \n",
    "                       for state, factor in df.nsmallest(3, 'price_factor')[['state', 'price_factor']].values)\n",
    "        )\n",
    "        \n",
    "        ax2.text(0.05, 0.95, summary_text, \n",
    "                transform=ax2.transAxes, \n",
    "                verticalalignment='top',\n",
    "                fontfamily='monospace')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nDetailed Statistics:\")\n",
    "        print(df.sort_values('price_factor', ascending=False)\\\n",
    "              [['state', 'price_factor', 'avg_price', 'listing_count']]\\\n",
    "              .to_string(index=False))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df117b8-091a-4659-a552-63c9ed853ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_csv_files(\"data/\")\n",
    "print(result)\n",
    "visualize_state_prices('data/state_price_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5150a4-1a7c-4b5b-894c-577746a4441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_oldest_and_newest_posts(folder_path):\n",
    "    \"\"\"\n",
    "    Get the oldest and newest posts based on file names containing date ranges.\n",
    "    File format expected: cleaned-vehicles-YYYY-MM-DD-YYYY-MM-DD.csv\n",
    "    where the first date is the oldest post and second date is the newest post in that file.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the files.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the oldest and newest posts with their corresponding files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"The folder '{folder_path}' does not exist.\")\n",
    "    \n",
    "    # Get all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith('cleaned-vehicles-') and f.endswith('.csv')]\n",
    "    \n",
    "    if not files:\n",
    "        return {\"oldest\": None, \"newest\": None}\n",
    "    \n",
    "    # Extract dates from file names\n",
    "    posts = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Remove prefix and suffix\n",
    "            date_part = file.replace('cleaned-vehicles-', '').replace('.csv', '')\n",
    "            \n",
    "            # Split into two dates\n",
    "            dates = date_part.split('-')\n",
    "            \n",
    "            # Reconstruct the dates (assuming YYYY-MM-DD-YYYY-MM-DD format)\n",
    "            first_date_str = f\"{dates[0]}-{dates[1]}-{dates[2]}\"\n",
    "            second_date_str = f\"{dates[3]}-{dates[4]}-{dates[5]}\"\n",
    "            \n",
    "            # Convert to datetime objects\n",
    "            first_date = datetime.strptime(first_date_str, \"%Y-%m-%d\")\n",
    "            last_date = datetime.strptime(second_date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            posts.append({\n",
    "                \"file\": file,\n",
    "                \"first_date\": first_date,\n",
    "                \"last_date\": last_date\n",
    "            })\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Skipping file with invalid format: {file}. Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not posts:\n",
    "        return {\"oldest\": None, \"newest\": None}\n",
    "    \n",
    "    # Find the file with the earliest first date and the latest last date\n",
    "    oldest_post = min(posts, key=lambda x: x[\"first_date\"])\n",
    "    newest_post = max(posts, key=lambda x: x[\"last_date\"])\n",
    "    \n",
    "    return {\n",
    "        \"oldest\": {\n",
    "            \"file\": oldest_post[\"file\"],\n",
    "            \"first_date\": oldest_post[\"first_date\"].strftime(\"%Y-%m-%d\"),\n",
    "        },\n",
    "        \"newest\": {\n",
    "            \"file\": newest_post[\"file\"],\n",
    "            \"last_date\": newest_post[\"last_date\"].strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def normalize_date(date_str, oldest_date_str, newest_date_str):\n",
    "    \"\"\"\n",
    "    Normalize a date to a value between -1 and 1, where:\n",
    "    - oldest_date maps to -1\n",
    "    - newest_date maps to 1\n",
    "    \n",
    "    Parameters:\n",
    "    date_str (str): Date to normalize in YYYY-MM-DD format\n",
    "    oldest_date_str (str): Reference start date in YYYY-MM-DD format\n",
    "    newest_date_str (str): Reference end date in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized value between -1 and 1\n",
    "    \"\"\"\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    oldest_date = datetime.strptime(oldest_date_str, \"%Y-%m-%d\")\n",
    "    newest_date = datetime.strptime(newest_date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate the total range in days\n",
    "    total_range = (newest_date - oldest_date).days\n",
    "    if total_range == 0:\n",
    "        return 0  # If oldest and newest are the same date\n",
    "    \n",
    "    # Calculate where our date falls within that range\n",
    "    days_from_start = (date - oldest_date).days\n",
    "    \n",
    "    # Normalize to [-1, 1] range\n",
    "    normalized_value = (2 * days_from_start / total_range) - 1\n",
    "    \n",
    "    return normalized_value\n",
    "\n",
    "def get_normalized_date_range(folder_path):\n",
    "    \"\"\"\n",
    "    Get the date range and provide a normalization function for dates within that range.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the files\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (date_range_dict, normalizer_function)\n",
    "    \"\"\"\n",
    "    date_range = get_oldest_and_newest_posts(folder_path)\n",
    "    \n",
    "    if date_range[\"oldest\"] is None or date_range[\"newest\"] is None:\n",
    "        return date_range, None\n",
    "    \n",
    "    oldest_date = date_range[\"oldest\"][\"first_date\"]\n",
    "    newest_date = date_range[\"newest\"][\"last_date\"]\n",
    "    \n",
    "    # Create a partial function with the date range pre-set\n",
    "    from functools import partial\n",
    "    normalizer = partial(normalize_date, \n",
    "                        oldest_date_str=oldest_date,\n",
    "                        newest_date_str=newest_date)\n",
    "    \n",
    "    return date_range, normalizer\n",
    "\n",
    "def analyze_feature_impact_across_files(folder_path, normalizer=None, correlation_series=None):\n",
    "    \"\"\"\n",
    "    Analyzes the impact of features on state-normalized price across multiple files,\n",
    "    or visualizes existing correlation series.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to folder containing the vehicle data CSV files\n",
    "    normalizer (DataNormalizer, optional): Pre-fitted DataNormalizer instance\n",
    "    correlation_series (pd.Series, optional): Existing correlation series to visualize\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (correlation_series, normalizer) - The correlation results and the normalizer instance used\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def visualize_correlations(correlation_series):\n",
    "        \"\"\"Helper function to visualize correlation series\"\"\"\n",
    "        # Sort correlations from smallest to largest\n",
    "        correlation_series = correlation_series.sort_values()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=correlation_series.index, y=correlation_series.values)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title(\"Feature Correlation with State-Normalized Price\")\n",
    "        plt.ylabel(\"Correlation Coefficient\")\n",
    "        plt.xlabel(\"Feature\")\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        # Calculate dynamic offset based on data range\n",
    "        y_range = correlation_series.max() - correlation_series.min()\n",
    "        offset = y_range * 0.01  # 1% of total range\n",
    "        \n",
    "        # Add correlation values on top of bars\n",
    "        for i, v in enumerate(correlation_series):\n",
    "            plt.text(i, v + (offset if v >= 0 else -offset),\n",
    "                    f'{v:.3f}',\n",
    "                    ha='center',\n",
    "                    va='bottom' if v >= 0 else 'top',\n",
    "                    rotation=90)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print correlations\n",
    "    print(\"\\nFeature correlations with state-normalized price:\")\n",
    "    for feature, corr in correlation_series.items():\n",
    "        print(f\"{feature:15} : {corr:>8.3f}\")\n",
    "    \n",
    "    # If correlation_series is provided, just visualize it and return\n",
    "    if correlation_series is not None:\n",
    "        visualize_correlations(correlation_series)\n",
    "        return correlation_series, normalizer\n",
    "    \n",
    "    # Load state price factors first\n",
    "    state_factors_path = os.path.join(folder_path, 'state_price_factors.csv')\n",
    "    if not os.path.exists(state_factors_path):\n",
    "        print(\"State price factors file not found. Please run process_csv_files first.\")\n",
    "        return None, normalizer\n",
    "    \n",
    "    state_factors = pd.read_csv(state_factors_path)\n",
    "    \n",
    "    # Get list of cleaned vehicle files\n",
    "    file_paths = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No files found matching the pattern 'cleaned-vehicles-*.csv'\")\n",
    "        return None, normalizer\n",
    "    \n",
    "    # Initialize correlation tracking\n",
    "    correlations_list = []\n",
    "    \n",
    "    # Columns to completely exclude from analysis\n",
    "    exclude_columns = [\n",
    "        'id', 'url', 'region', 'region_url', 'price', 'lat', 'long',\n",
    "        'posting_date', 'image_url', 'county', 'description', 'VIN',\n",
    "        'state', 'normalized_price', 'state_normalized_price', 'Unnamed: 0'  # Added Unnamed: 0\n",
    "    ]\n",
    "    \n",
    "    def adjust_price_for_state(df):\n",
    "        \"\"\"Helper function to adjust prices based on state factors\"\"\"\n",
    "        df = df.copy()\n",
    "        state_price_map = state_factors.set_index('state')['price_factor'].to_dict()\n",
    "        df['state_normalized_price'] = df.apply(\n",
    "            lambda row: row['price'] / (1 + state_price_map.get(row['state'], 0)/100), \n",
    "            axis=1\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read the file without index column\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Normalize prices by state first\n",
    "            df = adjust_price_for_state(df)\n",
    "            \n",
    "            # Drop excluded columns\n",
    "            analysis_df = df.drop(columns=[col for col in exclude_columns if col in df.columns])\n",
    "            \n",
    "            # Calculate correlations for each feature\n",
    "            for column in analysis_df.columns:\n",
    "                if column != 'state_normalized_price':\n",
    "                    # Skip non-numeric columns and convert categorical if possible\n",
    "                    if not np.issubdtype(analysis_df[column].dtype, np.number):\n",
    "                        if analysis_df[column].dtype == 'object':\n",
    "                            # Try to convert categorical to numeric using label encoding\n",
    "                            try:\n",
    "                                numeric_values = pd.Categorical(analysis_df[column]).codes\n",
    "                                if len(set(numeric_values)) > 1:  # Only if we have multiple categories\n",
    "                                    corr = np.corrcoef(\n",
    "                                        numeric_values, \n",
    "                                        df['state_normalized_price'].values\n",
    "                                    )[0,1]\n",
    "                                    if not np.isnan(corr):\n",
    "                                        correlations_list.append({'feature': column, 'correlation': corr})\n",
    "                            except:\n",
    "                                continue\n",
    "                    else:\n",
    "                        # For numeric columns, calculate correlation directly\n",
    "                        valid_data = analysis_df[[column]].join(df['state_normalized_price']).dropna()\n",
    "                        if len(valid_data) > 100:  # Only if we have enough data points\n",
    "                            corr = valid_data[column].corr(valid_data['state_normalized_price'])\n",
    "                            if not np.isnan(corr):\n",
    "                                correlations_list.append({'feature': column, 'correlation': corr})\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not correlations_list:\n",
    "        print(\"No correlations were calculated. Check if the data contains numeric columns.\")\n",
    "        return None, normalizer\n",
    "    \n",
    "    # Average correlations across files\n",
    "    correlation_df = pd.DataFrame(correlations_list)\n",
    "    correlation_series = correlation_df.groupby('feature')['correlation'].mean()\n",
    "    \n",
    "    # Sort by correlation value (ascending)\n",
    "    correlation_series = correlation_series.sort_values()\n",
    "    \n",
    "    # Visualize the correlations\n",
    "    visualize_correlations(correlation_series)\n",
    "    \n",
    "    return correlation_series, normalizer\n",
    "\n",
    "class DataNormalizer:\n",
    "    def __init__(self):\n",
    "        # Ordinal mappings\n",
    "        self.size_order = {\n",
    "            'sub-compact': 0,\n",
    "            'compact': 1,\n",
    "            'mid-size': 2,\n",
    "            'full-size': 3\n",
    "        }\n",
    "        \n",
    "        self.cylinder_order = {\n",
    "            'other': 0,\n",
    "            '3 cylinders': 3,\n",
    "            '4 cylinders': 4,\n",
    "            '5 cylinders': 5,\n",
    "            '6 cylinders': 6,\n",
    "            '8 cylinders': 8,\n",
    "            '10 cylinders': 10,\n",
    "            '12 cylinders': 12\n",
    "        }\n",
    "        \n",
    "        self.condition_order = {\n",
    "            'salvage': 0,\n",
    "            'fair': 1,\n",
    "            'good': 2,\n",
    "            'excellent': 3,\n",
    "            'like new': 4,\n",
    "            'new': 5\n",
    "        }\n",
    "        \n",
    "        # Reverse mappings for denormalization\n",
    "        self.size_reverse = {v: k for k, v in self.size_order.items()}\n",
    "        self.cylinder_reverse = {v: k for k, v in self.cylinder_order.items()}\n",
    "        self.condition_reverse = {v: k for k, v in self.condition_order.items()}\n",
    "        \n",
    "        # Storage for normalization parameters\n",
    "        self.numeric_params = {}\n",
    "        self.categorical_mappings = {}\n",
    "        \n",
    "        # Storage for imputation mappings\n",
    "        self.model_cylinders_map = {}\n",
    "        self.manufacturer_cylinders_map = {}\n",
    "        self.manufacturer_model_map = {}\n",
    "        self.most_common_cylinders = None\n",
    "        self.most_common_model = None\n",
    "        \n",
    "        # Define column types\n",
    "        self.numeric_cols = ['price', 'year', 'odometer']\n",
    "        self.categorical_cols = [\n",
    "            'fuel', 'title_status', 'transmission', \n",
    "            'drive', 'type', 'state', 'county'\n",
    "        ]\n",
    "        self.encode_only_cols = ['paint_color', 'region', 'manufacturer', 'model']\n",
    "\n",
    "    def fit(self, file_paths):\n",
    "        \"\"\"\n",
    "        Calculate normalization parameters and imputation mappings from multiple files.\n",
    "        \n",
    "        Parameters:\n",
    "        file_paths (list): List of paths to CSV files\n",
    "        \"\"\"\n",
    "        print(\"Calculating normalization parameters and imputation mappings...\")\n",
    "        \n",
    "        # Initialize parameters for numeric columns\n",
    "        numeric_mins = {col: float('inf') for col in self.numeric_cols}\n",
    "        numeric_maxs = {col: float('-inf') for col in self.numeric_cols}\n",
    "        \n",
    "        # Initialize sets for categorical values\n",
    "        categorical_values = {col: set() for col in self.categorical_cols}\n",
    "        encode_only_values = {col: set() for col in self.encode_only_cols}\n",
    "        \n",
    "        # Initialize counters for imputation\n",
    "        model_cylinders_count = {}      # {model: {cylinders: count}}\n",
    "        manufacturer_cylinders_count = {}  # {manufacturer: {cylinders: count}}\n",
    "        manufacturer_model_count = {}    # {manufacturer: {model: count}}\n",
    "        cylinders_count = {}            # {cylinders: count}\n",
    "        model_count = {}                # {model: count}\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "            \n",
    "            # Read file in chunks to save memory\n",
    "            chunk_size = 10000\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                # Update numeric min/max\n",
    "                for col in self.numeric_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        chunk_min = chunk[col].dropna().min()\n",
    "                        chunk_max = chunk[col].dropna().max()\n",
    "                        if not pd.isna(chunk_min) and not pd.isna(chunk_max):\n",
    "                            numeric_mins[col] = min(numeric_mins[col], chunk_min)\n",
    "                            numeric_maxs[col] = max(numeric_maxs[col], chunk_max)\n",
    "                \n",
    "                # Update categorical values\n",
    "                for col in self.categorical_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        categorical_values[col].update(chunk[col].dropna().unique())\n",
    "                \n",
    "                # Update encode-only values\n",
    "                for col in self.encode_only_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        encode_only_values[col].update(chunk[col].dropna().unique())\n",
    "                \n",
    "                # Count relationships for imputation\n",
    "                valid_rows = chunk[['manufacturer', 'model', 'cylinders']].dropna(subset=['manufacturer'])\n",
    "                \n",
    "                for _, row in valid_rows.iterrows():\n",
    "                    mfr = row['manufacturer']\n",
    "                    model = row['model']\n",
    "                    cyls = row['cylinders']\n",
    "                    \n",
    "                    # Count cylinders by model\n",
    "                    if pd.notna(model) and pd.notna(cyls):\n",
    "                        if model not in model_cylinders_count:\n",
    "                            model_cylinders_count[model] = {}\n",
    "                        model_cylinders_count[model][cyls] = model_cylinders_count[model].get(cyls, 0) + 1\n",
    "                    \n",
    "                    # Count cylinders by manufacturer\n",
    "                    if pd.notna(cyls):\n",
    "                        if mfr not in manufacturer_cylinders_count:\n",
    "                            manufacturer_cylinders_count[mfr] = {}\n",
    "                        manufacturer_cylinders_count[mfr][cyls] = manufacturer_cylinders_count[mfr].get(cyls, 0) + 1\n",
    "                        cylinders_count[cyls] = cylinders_count.get(cyls, 0) + 1\n",
    "                    \n",
    "                    # Count models by manufacturer\n",
    "                    if pd.notna(model):\n",
    "                        if mfr not in manufacturer_model_count:\n",
    "                            manufacturer_model_count[mfr] = {}\n",
    "                        manufacturer_model_count[mfr][model] = manufacturer_model_count[mfr].get(model, 0) + 1\n",
    "                        model_count[model] = model_count.get(model, 0) + 1\n",
    "        \n",
    "        # Store numeric parameters\n",
    "        for col in self.numeric_cols:\n",
    "            if numeric_mins[col] < numeric_maxs[col]:\n",
    "                self.numeric_params[col] = {\n",
    "                    'min': numeric_mins[col],\n",
    "                    'max': numeric_maxs[col]\n",
    "                }\n",
    "        \n",
    "        # Create categorical mappings\n",
    "        for col in self.categorical_cols:\n",
    "            if categorical_values[col]:\n",
    "                sorted_values = sorted(categorical_values[col])\n",
    "                mapping = {val: idx for idx, val in enumerate(sorted_values)}\n",
    "                reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "                self.categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': reverse_mapping,\n",
    "                    'max': len(mapping) - 1\n",
    "                }\n",
    "        \n",
    "        # Create encode-only mappings\n",
    "        for col in self.encode_only_cols:\n",
    "            if encode_only_values[col]:\n",
    "                sorted_values = sorted(encode_only_values[col])\n",
    "                mapping = {val: idx for idx, val in enumerate(sorted_values)}\n",
    "                reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "                self.categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': reverse_mapping\n",
    "                }\n",
    "        \n",
    "        # Create imputation mappings\n",
    "        # Most common cylinders by model\n",
    "        for model, cyls_dict in model_cylinders_count.items():\n",
    "            self.model_cylinders_map[model] = max(cyls_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Most common cylinders by manufacturer\n",
    "        for mfr, cyls_dict in manufacturer_cylinders_count.items():\n",
    "            self.manufacturer_cylinders_map[mfr] = max(cyls_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Most common model by manufacturer\n",
    "        for mfr, model_dict in manufacturer_model_count.items():\n",
    "            self.manufacturer_model_map[mfr] = max(model_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Global most common values\n",
    "        if cylinders_count:\n",
    "            self.most_common_cylinders = max(cylinders_count.items(), key=lambda x: x[1])[0]\n",
    "        if model_count:\n",
    "            self.most_common_model = max(model_count.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        print(\"Normalization parameters and imputation mappings calculated.\")\n",
    "        \n",
    "        # Print encoding ranges for encode-only columns\n",
    "        print(\"\\nEncoding ranges:\")\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in self.categorical_mappings:\n",
    "                n_values = len(self.categorical_mappings[col]['mapping'])\n",
    "                print(f\"{col}: 0 to {n_values - 1} ({n_values} unique values)\")\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\"\n",
    "        Normalize the DataFrame using stored parameters, with smart imputation for missing values.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Input DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Normalized DataFrame with imputed missing values\n",
    "        \"\"\"\n",
    "        if not self.numeric_params and not self.categorical_mappings:\n",
    "            raise ValueError(\"Normalizer has not been fitted. Call fit() first.\")\n",
    "            \n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        # Impute missing cylinders\n",
    "        mask = df_norm['cylinders'].isna()\n",
    "        if mask.any():\n",
    "            for idx in df_norm[mask].index:\n",
    "                model = df_norm.loc[idx, 'model']\n",
    "                manufacturer = df_norm.loc[idx, 'manufacturer']\n",
    "                \n",
    "                # Try to fill based on model\n",
    "                if pd.notna(model) and model in self.model_cylinders_map:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.model_cylinders_map[model]\n",
    "                # Try to fill based on manufacturer\n",
    "                elif pd.notna(manufacturer) and manufacturer in self.manufacturer_cylinders_map:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.manufacturer_cylinders_map[manufacturer]\n",
    "                # Use global most common\n",
    "                elif self.most_common_cylinders is not None:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.most_common_cylinders\n",
    "        \n",
    "        # Impute missing models\n",
    "        mask = df_norm['model'].isna()\n",
    "        if mask.any():\n",
    "            for idx in df_norm[mask].index:\n",
    "                manufacturer = df_norm.loc[idx, 'manufacturer']\n",
    "                \n",
    "                # Try to fill based on manufacturer\n",
    "                if pd.notna(manufacturer) and manufacturer in self.manufacturer_model_map:\n",
    "                    df_norm.loc[idx, 'model'] = self.manufacturer_model_map[manufacturer]\n",
    "                # Use global most common\n",
    "                elif self.most_common_model is not None:\n",
    "                    df_norm.loc[idx, 'model'] = self.most_common_model\n",
    "        \n",
    "        # Normalize numeric columns\n",
    "        for col in self.numeric_cols:\n",
    "            if col in df_norm.columns and col in self.numeric_params:\n",
    "                params = self.numeric_params[col]\n",
    "                df_norm[col] = (df_norm[col] - params['min']) / (params['max'] - params['min'])\n",
    "        \n",
    "        # Normalize ordinal columns\n",
    "        # Size\n",
    "        if 'size' in df_norm.columns:\n",
    "            df_norm['size'] = df_norm['size'].map(self.size_order)\n",
    "            size_max = max(self.size_order.values())\n",
    "            df_norm['size'] = df_norm['size'] / size_max\n",
    "        \n",
    "        # Cylinders\n",
    "        if 'cylinders' in df_norm.columns:\n",
    "            df_norm['cylinders'] = df_norm['cylinders'].map(self.cylinder_order)\n",
    "            cylinder_max = max(self.cylinder_order.values())\n",
    "            df_norm['cylinders'] = df_norm['cylinders'] / cylinder_max\n",
    "        \n",
    "        # Condition\n",
    "        if 'condition' in df_norm.columns:\n",
    "            df_norm['condition'] = df_norm['condition'].map(self.condition_order)\n",
    "            condition_max = max(self.condition_order.values())\n",
    "            df_norm['condition'] = df_norm['condition'] / condition_max\n",
    "        \n",
    "        # Normalize categorical columns to 0-1\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_norm.columns and col in self.categorical_mappings:\n",
    "                mapping = self.categorical_mappings[col]['mapping']\n",
    "                max_val = self.categorical_mappings[col]['max']\n",
    "                df_norm[col] = df_norm[col].map(mapping) / max_val\n",
    "        \n",
    "        # Handle encode-only columns (simple integer encoding)\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in df_norm.columns and col in self.categorical_mappings:\n",
    "                mapping = self.categorical_mappings[col]['mapping']\n",
    "                df_norm[col] = df_norm[col].map(mapping)\n",
    "        \n",
    "        return df_norm\n",
    "\n",
    "    def denormalize(self, df_norm):\n",
    "        \"\"\"\n",
    "        Denormalize the DataFrame using stored parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        df_norm (pandas.DataFrame): Normalized DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Denormalized DataFrame with original scale and categories\n",
    "        \"\"\"\n",
    "        if not self.numeric_params and not self.categorical_mappings:\n",
    "            raise ValueError(\"Normalizer has not been fitted. Call fit() first.\")\n",
    "            \n",
    "        df_denorm = df_norm.copy()\n",
    "        \n",
    "        # Denormalize numeric columns\n",
    "        for col in self.numeric_cols:\n",
    "            if col in df_denorm.columns and col in self.numeric_params:\n",
    "                params = self.numeric_params[col]\n",
    "                df_denorm[col] = df_denorm[col] * (params['max'] - params['min']) + params['min']\n",
    "        \n",
    "        # Denormalize ordinal columns\n",
    "        # Size\n",
    "        if 'size' in df_denorm.columns:\n",
    "            size_max = max(self.size_order.values())\n",
    "            df_denorm['size'] = (df_denorm['size'] * size_max).round()\n",
    "            df_denorm['size'] = df_denorm['size'].map(self.size_reverse)\n",
    "        \n",
    "        # Cylinders\n",
    "        if 'cylinders' in df_denorm.columns:\n",
    "            cylinder_max = max(self.cylinder_order.values())\n",
    "            df_denorm['cylinders'] = (df_denorm['cylinders'] * cylinder_max).round()\n",
    "            df_denorm['cylinders'] = df_denorm['cylinders'].map(self.cylinder_reverse)\n",
    "        \n",
    "        # Condition\n",
    "        if 'condition' in df_denorm.columns:\n",
    "            condition_max = max(self.condition_order.values())\n",
    "            df_denorm['condition'] = (df_denorm['condition'] * condition_max).round()\n",
    "            df_denorm['condition'] = df_denorm['condition'].map(self.condition_reverse)\n",
    "        \n",
    "        # Denormalize categorical columns\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_denorm.columns and col in self.categorical_mappings:\n",
    "                max_val = self.categorical_mappings[col]['max']\n",
    "                reverse_mapping = self.categorical_mappings[col]['reverse_mapping']\n",
    "                df_denorm[col] = (df_denorm[col] * max_val).round()\n",
    "                df_denorm[col] = df_denorm[col].map(reverse_mapping)\n",
    "        \n",
    "        # Handle encode-only columns\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in df_denorm.columns and col in self.categorical_mappings:\n",
    "                reverse_mapping = self.categorical_mappings[col]['reverse_mapping']\n",
    "                df_denorm[col] = df_denorm[col].round()\n",
    "                df_denorm[col] = df_denorm[col].map(reverse_mapping)\n",
    "        \n",
    "        return df_denorm\n",
    "\n",
    "\n",
    "\n",
    "def analyze_feature_impact_by_model(folder_path, normalizer=None):\n",
    "    \"\"\"\n",
    "    Analyzes feature impact on price for each car model separately,\n",
    "    with data normalization and robust handling of missing values.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing vehicle data CSV files\n",
    "    normalizer (DataNormalizer, optional): Pre-fitted normalizer instance\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing correlation results by model\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from glob import glob\n",
    "    import os\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress numpy warnings about division\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    # Initialize or use provided normalizer\n",
    "    if normalizer is None:\n",
    "        normalizer = DataNormalizer()\n",
    "        # Get all file paths for fitting\n",
    "        file_paths = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "        if file_paths:\n",
    "            print(\"Fitting normalizer on all data...\")\n",
    "            normalizer.fit(file_paths)\n",
    "        else:\n",
    "            print(\"No files found for fitting normalizer\")\n",
    "            return None\n",
    "    \n",
    "    # Columns to exclude from analysis\n",
    "    exclude_columns = [\n",
    "        'id', 'url', 'region_url', 'image_url', 'description', \n",
    "        'manufacturer', 'model', 'VIN', 'posting_date'\n",
    "    ]\n",
    "    \n",
    "    def safe_correlation(x, y):\n",
    "        \"\"\"Calculate correlation with proper handling of edge cases\"\"\"\n",
    "        # Remove any null values\n",
    "        mask = ~(np.isnan(x) | np.isnan(y))\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        \n",
    "        # Need at least 3 points for meaningful correlation\n",
    "        if len(x) < 3:\n",
    "            return np.nan\n",
    "            \n",
    "        # Check for zero variance\n",
    "        if np.std(x) == 0 or np.std(y) == 0:\n",
    "            return np.nan\n",
    "            \n",
    "        try:\n",
    "            return np.corrcoef(x, y)[0, 1]\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    # Initialize storage for model-specific correlations\n",
    "    model_correlations = {}\n",
    "    \n",
    "    # Process each file\n",
    "    file_paths = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No files found matching the pattern 'cleaned-vehicles-*.csv'\")\n",
    "        return None\n",
    "        \n",
    "    # Process each file\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read the file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Process each model that has sufficient data\n",
    "            for model in df['model'].unique():\n",
    "                if pd.isna(model):\n",
    "                    continue\n",
    "                    \n",
    "                # Get data for this model\n",
    "                model_df = df[df['model'] == model].copy()\n",
    "                \n",
    "                # Require at least 50 samples for reliable correlation\n",
    "                if len(model_df) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # Normalize the data\n",
    "                try:\n",
    "                    normalized_df = normalizer.normalize(model_df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error normalizing data for model {model}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize correlations for this model if not exists\n",
    "                if model not in model_correlations:\n",
    "                    model_correlations[model] = {\n",
    "                        'correlations': {},\n",
    "                        'sample_size': 0,\n",
    "                        'significant_correlations': {}\n",
    "                    }\n",
    "                \n",
    "                # Update sample size\n",
    "                model_correlations[model]['sample_size'] += len(normalized_df)\n",
    "                \n",
    "                # Calculate correlations for normalized features\n",
    "                for column in normalized_df.columns:\n",
    "                    if column in exclude_columns or column == 'price' or column == 'model':\n",
    "                        continue\n",
    "                    \n",
    "                    # Handle numeric columns (all should be numeric after normalization)\n",
    "                    if pd.api.types.is_numeric_dtype(normalized_df[column]):\n",
    "                        correlation = safe_correlation(\n",
    "                            normalized_df[column].astype(float), \n",
    "                            normalized_df['price'].astype(float)\n",
    "                        )\n",
    "                        \n",
    "                        if not np.isnan(correlation):\n",
    "                            # Update running average of correlations with weight by sample size\n",
    "                            if column not in model_correlations[model]['correlations']:\n",
    "                                model_correlations[model]['correlations'][column] = correlation\n",
    "                            else:\n",
    "                                prev_corr = model_correlations[model]['correlations'][column]\n",
    "                                prev_samples = model_correlations[model]['sample_size'] - len(normalized_df)\n",
    "                                new_corr = (prev_corr * prev_samples + correlation * len(normalized_df)) / model_correlations[model]['sample_size']\n",
    "                                model_correlations[model]['correlations'][column] = new_corr\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Filter for significant correlations and sort\n",
    "    for model in model_correlations:\n",
    "        # Consider correlations significant if abs value >= 0.1\n",
    "        significant = {k: v for k, v in model_correlations[model]['correlations'].items() \n",
    "                      if abs(v) >= 0.1}\n",
    "        model_correlations[model]['significant_correlations'] = dict(\n",
    "            sorted(significant.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        )\n",
    "    \n",
    "    return model_correlations, normalizer\n",
    "\n",
    "def visualize_model_correlations(model_correlations, min_samples=100, top_n_models=10):\n",
    "    \"\"\"\n",
    "    Visualize the correlations for top models with sufficient samples.\n",
    "    \n",
    "    Parameters:\n",
    "    model_correlations (dict): Output from analyze_feature_impact_by_model\n",
    "    min_samples (int): Minimum number of samples required for visualization\n",
    "    top_n_models (int): Number of top models to display\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Filter models with sufficient samples\n",
    "    valid_models = {model: data for model, data in model_correlations.items() \n",
    "                   if data['sample_size'] >= min_samples}\n",
    "    \n",
    "    if not valid_models:\n",
    "        print(f\"No models found with at least {min_samples} samples\")\n",
    "        return\n",
    "    \n",
    "    # Sort models by sample size and take top N\n",
    "    top_models = dict(sorted(valid_models.items(), \n",
    "                           key=lambda x: x[1]['sample_size'], \n",
    "                           reverse=True)[:top_n_models])\n",
    "    \n",
    "    for model, data in top_models.items():\n",
    "        if not data['significant_correlations']:\n",
    "            continue\n",
    "            \n",
    "        # Create figure for this model\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get correlations and feature names\n",
    "        correlations = data['significant_correlations']\n",
    "        features = list(correlations.keys())\n",
    "        values = list(correlations.values())\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.barh(range(len(features)), \n",
    "                       [abs(v) for v in values],\n",
    "                       color=[plt.cm.RdBu(0.5 * (v + 1)) for v in values])\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.yticks(range(len(features)), features, fontsize=8)\n",
    "        plt.xlabel('Absolute Correlation with Normalized Price')\n",
    "        plt.title(f'Feature Impact for {model}\\n(n={data[\"sample_size\"]:,} samples)')\n",
    "        \n",
    "        # Add correlation values on bars\n",
    "        for i, v in enumerate(values):\n",
    "            plt.text(abs(v), i, f'{v:.3f}', \n",
    "                    va='center', fontsize=8,\n",
    "                    color='black' if abs(v) < 0.5 else 'white')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop correlations for {model} (n={data['sample_size']:,}):\")\n",
    "        for feature, corr in data['significant_correlations'].items():\n",
    "            print(f\"{feature:30} : {corr:>8.3f}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "def analyze_overall_feature_impact(folder_path, normalizer=None):\n",
    "    \"\"\"\n",
    "    Analyzes feature impact on price across all cars, with proper normalization\n",
    "    and robust handling of outliers.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing vehicle data CSV files\n",
    "    normalizer (DataNormalizer, optional): Pre-fitted normalizer instance\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (overall_correlations, sample_sizes, normalizer)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from glob import glob\n",
    "    import os\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    # Initialize or use provided normalizer\n",
    "    if normalizer is None:\n",
    "        normalizer = DataNormalizer()\n",
    "        file_paths = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "        if file_paths:\n",
    "            print(\"Fitting normalizer on all data...\")\n",
    "            normalizer.fit(file_paths)\n",
    "        else:\n",
    "            print(\"No files found for fitting normalizer\")\n",
    "            return None\n",
    "    \n",
    "    # Columns to exclude\n",
    "    exclude_columns = [\n",
    "        'id', 'url', 'region_url', 'image_url', 'description', \n",
    "        'model', 'VIN', 'posting_date', 'manufacturer'\n",
    "    ]\n",
    "    \n",
    "    # Initialize storage for overall statistics\n",
    "    correlations = {}\n",
    "    sample_sizes = {}\n",
    "    \n",
    "    # Process each file\n",
    "    file_paths = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No files found matching the pattern 'cleaned-vehicles-*.csv'\")\n",
    "        return None\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read and normalize the data\n",
    "            df = pd.read_csv(file_path)\n",
    "            normalized_df = normalizer.normalize(df)\n",
    "            \n",
    "            # Calculate correlations for each feature\n",
    "            for column in normalized_df.columns:\n",
    "                if column in exclude_columns or column == 'price':\n",
    "                    continue\n",
    "                \n",
    "                if pd.api.types.is_numeric_dtype(normalized_df[column]):\n",
    "                    # Remove nulls and outliers\n",
    "                    data = pd.DataFrame({\n",
    "                        'feature': normalized_df[column],\n",
    "                        'price': normalized_df['price']\n",
    "                    }).dropna()\n",
    "                    \n",
    "                    # Remove outliers using IQR method\n",
    "                    Q1 = data['feature'].quantile(0.25)\n",
    "                    Q3 = data['feature'].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    mask = ~((data['feature'] < (Q1 - 1.5 * IQR)) | \n",
    "                            (data['feature'] > (Q3 + 1.5 * IQR)))\n",
    "                    clean_data = data[mask]\n",
    "                    \n",
    "                    if len(clean_data) >= 50:  # Minimum sample size\n",
    "                        correlation = clean_data['feature'].corr(clean_data['price'])\n",
    "                        \n",
    "                        if not np.isnan(correlation):\n",
    "                            # Update weighted average correlation\n",
    "                            if column not in correlations:\n",
    "                                correlations[column] = correlation\n",
    "                                sample_sizes[column] = len(clean_data)\n",
    "                            else:\n",
    "                                total_n = sample_sizes[column] + len(clean_data)\n",
    "                                weight1 = sample_sizes[column] / total_n\n",
    "                                weight2 = len(clean_data) / total_n\n",
    "                                correlations[column] = (correlations[column] * weight1 + \n",
    "                                                      correlation * weight2)\n",
    "                                sample_sizes[column] = total_n\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort correlations by absolute value\n",
    "    sorted_correlations = dict(sorted(correlations.items(), \n",
    "                                    key=lambda x: abs(x[1]), \n",
    "                                    reverse=True))\n",
    "    \n",
    "    return sorted_correlations, sample_sizes, normalizer\n",
    "\n",
    "def visualize_overall_correlations(correlations, sample_sizes):\n",
    "    \"\"\"\n",
    "    Visualize the overall feature correlations with price.\n",
    "    \n",
    "    Parameters:\n",
    "    correlations (dict): Feature correlations with price\n",
    "    sample_sizes (dict): Sample sizes for each feature\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Get features and correlation values\n",
    "    features = list(correlations.keys())\n",
    "    values = list(correlations.values())\n",
    "    \n",
    "    # Create color map based on correlation values\n",
    "    colors = [plt.cm.RdBu(0.5 * (v + 1)) for v in values]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.barh(range(len(features)), [abs(v) for v in values], color=colors)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.yticks(range(len(features)), \n",
    "               [f\"{f} (n={sample_sizes[f]:,})\" for f in features], \n",
    "               fontsize=10)\n",
    "    plt.xlabel('Absolute Correlation with Price')\n",
    "    plt.title('Overall Feature Impact on Car Prices\\n(Normalized Data)')\n",
    "    \n",
    "    # Add correlation values on bars\n",
    "    for i, v in enumerate(values):\n",
    "        plt.text(abs(v), i, f'{v:.3f}', \n",
    "                va='center', fontsize=10,\n",
    "                color='black' if abs(v) < 0.5 else 'white')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed correlation results:\")\n",
    "    print(\"\\nFeature                          Correlation    Sample Size\")\n",
    "    print(\"-\" * 60)\n",
    "    for feature, corr in correlations.items():\n",
    "        print(f\"{feature:30} : {corr:>8.3f}    n={sample_sizes[feature]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcda10-bbf6-4081-b3a2-c1df76906297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_series, normalizer = analyze_feature_impact_across_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600390df-8bfb-44e2-8ce6-d81e1503a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_series, normalizer = analyze_feature_impact_across_files(\"data\", normalizer, correlation_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4313d7-9ccc-4d0d-bf4d-3224a6af421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_correlations = analyze_feature_impact_by_model(\"data\")\n",
    "model_correlations, normalizer = model_correlations\n",
    "visualize_overall__correlations(model_correlations, min_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c888c-8405-449b-9515-0b577747e8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
