{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "To debug a cell, press Alt+Shift+Enter, or click !here goes the icon of the corresponding button in the gutter! button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/jupyter-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub\n",
    "#!pip install seaborn\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "path = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc27f8-eebe-43d1-8ad0-e007865bf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_car_age(df):\n",
    "    \"\"\"\n",
    "    Add a column calculating the difference between posting year and car year.\n",
    "    Uses the posting_year column that was already created by clean_car_data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'year' and 'posting_year' columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original dataframe with new 'car_age' column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate the age of the car at posting time\n",
    "    df['car_age'] = df['posting_year'] - df['year']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_and_clean_single_vehicle_file(file_path):\n",
    "    \"\"\"\n",
    "    Load, clean, normalize a vehicle data file, calculate car age, \n",
    "    and save the cleaned DataFrame with a name including the earliest \n",
    "    and latest posting dates.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the vehicle data CSV file to process\n",
    "    \n",
    "    Returns:\n",
    "    None: The cleaned DataFrame is written to a new CSV file\n",
    "    \"\"\"\n",
    "    print(\"Starting to process and clean the vehicle file...\")\n",
    "\n",
    "    # Extract the file name and determine posting_date if applicable\n",
    "    file_name = os.path.basename(file_path)\n",
    "    parts = file_name.replace('.csv', '').split('-')\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        year = parts[-1]\n",
    "        month = parts[-2]\n",
    "        default_posting_date = f\"{year}-{month}-01\"\n",
    "    else:\n",
    "        default_posting_date = None  # No default posting_date if the file doesn't follow the pattern\n",
    "    \n",
    "    # Load CSV into a DataFrame\n",
    "    print(f\"Reading file: {file_name}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Add posting_date column if it doesn't exist\n",
    "    if 'posting_date' not in df.columns:\n",
    "        df['posting_date'] = default_posting_date\n",
    "    \n",
    "    # Clean and normalize the DataFrame\n",
    "    print(\"Cleaning and normalizing data...\")\n",
    "    cleaned_df = clean_car_data(df)\n",
    "    \n",
    "    # Add car age\n",
    "    print(\"Adding car age column...\")\n",
    "    cleaned_df = add_car_age(cleaned_df)\n",
    "    \n",
    "    # Convert posting_date to datetime for sorting and range determination\n",
    "    cleaned_df['posting_date'] = pd.to_datetime(cleaned_df['posting_date'], errors='coerce')\n",
    "    earliest_date = cleaned_df['posting_date'].min().strftime('%Y-%m-%d') if not cleaned_df['posting_date'].isna().all() else 'unknown'\n",
    "    latest_date = cleaned_df['posting_date'].max().strftime('%Y-%m-%d') if not cleaned_df['posting_date'].isna().all() else 'unknown'\n",
    "    \n",
    "    # Construct the output file name\n",
    "    output_file_name = f\"cleaned-vehicles-{earliest_date}-{latest_date}.csv\"\n",
    "    output_path = os.path.join(os.path.dirname(file_path), output_file_name)\n",
    "    \n",
    "    # Write the cleaned DataFrame to the output file\n",
    "    print(f\"Writing cleaned data to: {output_file_name}\")\n",
    "    cleaned_df.to_csv(output_path, index=False)\n",
    "    print(\"Processing and cleaning complete.\")\n",
    "\n",
    "def analyze_and_normalize_data(df):\n",
    "    \"\"\"\n",
    "    Perform analysis and normalization of vehicle prices based on state-level factors.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Cleaned vehicle DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with normalized prices\n",
    "    \"\"\"\n",
    "    print(\"Calculating state price factors...\")\n",
    "    state_factors = calculate_state_price_factors(df)\n",
    "    print(\"State price factors calculated.\")\n",
    "    \n",
    "    print(\"Adding normalized prices...\")\n",
    "    df_with_normalized = add_normalized_prices(df)\n",
    "    print(\"Normalized prices added successfully.\")\n",
    "    \n",
    "    return df_with_normalized\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def clean_car_data(df):\n",
    "    \"\"\"\n",
    "    Clean car listing data by:\n",
    "    1. Removing rows with more than 2 missing values (ignoring specified columns)\n",
    "    2. Filling single missing values with medians from similar cars\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Raw car listing data\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Cleaned dataset\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Extract year from posting_date string\n",
    "    cleaned_df['posting_year'] = cleaned_df['posting_date'].str[:4].astype(float)\n",
    "    \n",
    "    # Columns to ignore in missing value calculations\n",
    "    ignore_columns = ['long', 'lat', 'image_url', 'region', 'region_url', 'county', 'state']\n",
    "    \n",
    "    # Get columns to check for missing values (excluding ignored ones)\n",
    "    columns_to_check = [col for col in cleaned_df.columns if col not in ignore_columns]\n",
    "    \n",
    "    # Remove rows with more than 2 missing values in the relevant columns\n",
    "    rows_to_keep = cleaned_df[columns_to_check].isnull().sum(axis=1) <= 2\n",
    "    cleaned_df = cleaned_df[rows_to_keep]\n",
    "    \n",
    "    # Function to fill missing values based on similar cars\n",
    "    def fill_missing_value(row, column):\n",
    "        if pd.isnull(row[column]):\n",
    "            # Get median value for same make, model, year posted in same year\n",
    "            similar_cars = cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (cleaned_df['manufacturer'] == row['manufacturer']) &\n",
    "                (cleaned_df['model'] == row['model']) &\n",
    "                (cleaned_df['posting_year'] == row['posting_year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ]\n",
    "            \n",
    "            if len(similar_cars) >= 3:  # If we have enough similar cars\n",
    "                return similar_cars[column].median()\n",
    "            \n",
    "            # If not enough similar cars, broaden criteria (ignore model)\n",
    "            similar_cars = cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (cleaned_df['manufacturer'] == row['manufacturer']) &\n",
    "                (cleaned_df['posting_year'] == row['posting_year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ]\n",
    "            \n",
    "            if len(similar_cars) >= 3:\n",
    "                return similar_cars[column].median()\n",
    "            \n",
    "            # If still not enough, use global median for that year\n",
    "            return cleaned_df[\n",
    "                (cleaned_df['year'] == row['year']) &\n",
    "                (~pd.isnull(cleaned_df[column]))\n",
    "            ][column].median()\n",
    "    \n",
    "    # Fill single missing values for numeric columns\n",
    "    numeric_columns = ['odometer', 'price']  # Add other numeric columns as needed\n",
    "    for column in numeric_columns:\n",
    "        mask = cleaned_df[column].isnull()\n",
    "        cleaned_df.loc[mask, column] = cleaned_df[mask].apply(\n",
    "            lambda row: fill_missing_value(row, column), axis=1\n",
    "        )\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def process_all_vehicle_files(folder_path):\n",
    "    \"\"\"\n",
    "    Process and clean all vehicle files in a given folder.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing vehicle data CSV files\n",
    "    \n",
    "    Returns:\n",
    "    None: Processed files are saved with new names in the same folder\n",
    "    \"\"\"\n",
    "    print(f\"Processing all vehicle files in folder: {folder_path}\")\n",
    "    \n",
    "    # Get a list of all CSV files in the folder starting with 'vehicles'\n",
    "    file_paths = glob(os.path.join(folder_path, \"vehicles*.csv\"))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"No files found in the folder matching the pattern 'vehicles*.csv'.\")\n",
    "        return\n",
    "    \n",
    "    for idx, file_path in enumerate(file_paths, 1):\n",
    "        print(f\"Processing file {idx}/{len(file_paths)}: {os.path.basename(file_path)}\")\n",
    "        try:\n",
    "            process_and_clean_single_vehicle_file(file_path)\n",
    "            print(f\"File {idx} processed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    print(\"All files in the folder have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce08bba-9065-40d3-b94f-aa06201cd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_all_vehicle_files(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634db1c1-164d-42ce-8e5f-573e22b6459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(input_folder):\n",
    "    \"\"\"\n",
    "    Processes all cleaned CSV files in the input folder, calculates state price factors,\n",
    "    and outputs the aggregated result to 'state_price_factors.csv'.\n",
    "\n",
    "    Parameters:\n",
    "    input_folder (str): Path to the folder containing the cleaned CSV files.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Aggregated state price factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_state_price_factors(df):\n",
    "        \"\"\"\n",
    "        Calculate how much each state's prices differ from the national average,\n",
    "        with outlier removal and minimum listing requirements.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Cleaned car listing DataFrame\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: State price factors showing percentage difference from national average\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create copy for analysis\n",
    "        df_analysis = df.copy()\n",
    "        \n",
    "        # Remove national outliers first\n",
    "        Q1 = df_analysis['price'].quantile(0.25)\n",
    "        Q3 = df_analysis['price'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        price_lower_bound = Q1 - 1.5 * IQR\n",
    "        price_upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_analysis = df_analysis[\n",
    "            (df_analysis['price'] >= price_lower_bound) &\n",
    "            (df_analysis['price'] <= price_upper_bound)\n",
    "        ]\n",
    "        \n",
    "        # Calculate national average after removing outliers\n",
    "        national_avg = df_analysis['price'].mean()\n",
    "        \n",
    "        # Function to remove outliers for a specific state\n",
    "        def get_state_mean_without_outliers(state_data):\n",
    "            if len(state_data) < 100:  # Minimum listings requirement\n",
    "                return None\n",
    "                \n",
    "            # Remove state-level outliers\n",
    "            Q1 = state_data['price'].quantile(0.25)\n",
    "            Q3 = state_data['price'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            \n",
    "            clean_state_data = state_data[\n",
    "                (state_data['price'] >= lower) &\n",
    "                (state_data['price'] <= upper)\n",
    "            ]\n",
    "            \n",
    "            if len(clean_state_data) < 50:  # Minimum cleaned listings requirement\n",
    "                return None\n",
    "                \n",
    "            return clean_state_data['price'].mean()\n",
    "        \n",
    "        # Calculate state-level statistics\n",
    "        state_stats = []\n",
    "        \n",
    "        for state in df_analysis['state'].unique():\n",
    "            state_data = df_analysis[df_analysis['state'] == state]\n",
    "            state_mean = get_state_mean_without_outliers(state_data)\n",
    "            \n",
    "            if state_mean is not None:\n",
    "                price_factor = (state_mean / national_avg - 1) * 100\n",
    "                \n",
    "                # Skip states with unrealistic price factors\n",
    "                if abs(price_factor) <= 50:  # Maximum allowed deviation\n",
    "                    state_stats.append({\n",
    "                        'state': state,\n",
    "                        'avg_price': state_mean,\n",
    "                        'price_factor': price_factor,\n",
    "                        'listing_count': len(state_data)\n",
    "                    })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        state_stats = pd.DataFrame(state_stats)\n",
    "        \n",
    "        # Sort by price factor\n",
    "        state_stats = state_stats.sort_values('price_factor', ascending=False)\n",
    "        \n",
    "        # Round numbers for readability\n",
    "        state_stats['avg_price'] = state_stats['avg_price'].round(2)\n",
    "        state_stats['price_factor'] = state_stats['price_factor'].round(2)\n",
    "        \n",
    "        return state_stats\n",
    "\n",
    "    # Find all cleaned CSV files\n",
    "    csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                 if f.startswith('cleaned-') and f.endswith('.csv')]\n",
    "    \n",
    "    # Aggregate data from all files\n",
    "    aggregated_df = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        print(f\"starting work on {file}\") \n",
    "        df = pd.read_csv(file)\n",
    "        aggregated_df = pd.concat([aggregated_df, df], ignore_index=True)\n",
    "    \n",
    "    # Calculate state price factors\n",
    "    state_price_factors = calculate_state_price_factors(aggregated_df)\n",
    "    \n",
    "    # Save the result to CSV\n",
    "    output_file = os.path.join(input_folder, 'state_price_factors.csv')\n",
    "    state_price_factors.to_csv(output_file, index=False)\n",
    "    \n",
    "    return state_price_factors\n",
    "\n",
    "def adjust_price_for_location(price, state, state_factors):\n",
    "    \"\"\"\n",
    "    Adjust a car's price to account for state-level price differences.\n",
    "    Returns original price if state adjustment isn't available.\n",
    "    \n",
    "    Parameters:\n",
    "    price (float): Original price\n",
    "    state (str): State where the car is listed\n",
    "    state_factors (pandas.DataFrame): DataFrame with state price factors\n",
    "    \n",
    "    Returns:\n",
    "    float: Nationally adjusted price\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the state's price factor\n",
    "        state_factor = state_factors.loc[state_factors['state'] == state, 'price_factor'].iloc[0]\n",
    "        \n",
    "        # Adjust price by removing state factor\n",
    "        adjusted_price = price / (1 + state_factor/100)\n",
    "        \n",
    "        return round(adjusted_price, 2)\n",
    "    except:\n",
    "        # Return original price if state adjustment isn't available\n",
    "        return price\n",
    "\n",
    "def add_normalized_prices(df):\n",
    "    \"\"\"\n",
    "    Add a normalized_price column to the DataFrame that adjusts for state-level price differences.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing car listings\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Original DataFrame with new normalized_price column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_normalized = df.copy()\n",
    "    \n",
    "    # Calculate state factors\n",
    "    state_factors = calculate_state_price_factors(df)\n",
    "    \n",
    "    # Create normalized_price column using vectorized operations\n",
    "    df_with_normalized['state_price_factor'] = df_with_normalized['state'].map(\n",
    "        state_factors.set_index('state')['price_factor'].fillna(0)\n",
    "    )\n",
    "    \n",
    "    df_with_normalized['normalized_price'] = df_with_normalized.apply(\n",
    "        lambda row: adjust_price_for_location(row['price'], row['state'], state_factors),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return df_with_normalized\n",
    "\n",
    "\n",
    "def visualize_state_prices(filename='state_price_factors.csv'):\n",
    "    \"\"\"\n",
    "    Read and visualize state price factors data.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Path to the state price factors CSV file\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Sort by price factor\n",
    "        df = df.sort_values('price_factor')\n",
    "        \n",
    "        # Create figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[2, 1])\n",
    "        \n",
    "        # Bar plot\n",
    "        sns.barplot(data=df, x='state', y='price_factor', ax=ax1)\n",
    "        ax1.set_title('State Price Differences from National Average')\n",
    "        ax1.set_xlabel('State')\n",
    "        ax1.set_ylabel('Price Difference (%)')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add horizontal line at 0\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        # Text summary\n",
    "        summary_text = (\n",
    "            f\"Summary Statistics:\\n\"\n",
    "            f\"Number of states: {len(df)}\\n\"\n",
    "            f\"Price factor range: {df['price_factor'].min():.1f}% to {df['price_factor'].max():.1f}%\\n\"\n",
    "            f\"Total listings analyzed: {df['listing_count'].sum():,}\\n\\n\"\n",
    "            f\"Top 3 most expensive states:\\n\"\n",
    "            + \"\\n\".join(f\"{state}: +{factor:.1f}%\" \n",
    "                       for state, factor in df.nlargest(3, 'price_factor')[['state', 'price_factor']].values)\n",
    "            + \"\\n\\nTop 3 least expensive states:\\n\"\n",
    "            + \"\\n\".join(f\"{state}: {factor:.1f}%\" \n",
    "                       for state, factor in df.nsmallest(3, 'price_factor')[['state', 'price_factor']].values)\n",
    "        )\n",
    "        \n",
    "        ax2.text(0.05, 0.95, summary_text, \n",
    "                transform=ax2.transAxes, \n",
    "                verticalalignment='top',\n",
    "                fontfamily='monospace')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nDetailed Statistics:\")\n",
    "        print(df.sort_values('price_factor', ascending=False)\\\n",
    "              [['state', 'price_factor', 'avg_price', 'listing_count']]\\\n",
    "              .to_string(index=False))\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error visualizing data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df117b8-091a-4659-a552-63c9ed853ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = process_csv_files(\"data/\")\n",
    "print(result)\n",
    "visualize_state_prices('data/state_price_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5150a4-1a7c-4b5b-894c-577746a4441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_oldest_and_newest_posts(folder_path):\n",
    "    \"\"\"\n",
    "    Get the oldest and newest posts based on file names containing date ranges.\n",
    "    File format expected: cleaned-vehicles-YYYY-MM-DD-YYYY-MM-DD.csv\n",
    "    where the first date is the oldest post and second date is the newest post in that file.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the files.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the oldest and newest posts with their corresponding files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"The folder '{folder_path}' does not exist.\")\n",
    "    \n",
    "    # Get all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.startswith('cleaned-vehicles-') and f.endswith('.csv')]\n",
    "    \n",
    "    if not files:\n",
    "        return {\"oldest\": None, \"newest\": None}\n",
    "    \n",
    "    # Extract dates from file names\n",
    "    posts = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            # Remove prefix and suffix\n",
    "            date_part = file.replace('cleaned-vehicles-', '').replace('.csv', '')\n",
    "            \n",
    "            # Split into two dates\n",
    "            dates = date_part.split('-')\n",
    "            \n",
    "            # Reconstruct the dates (assuming YYYY-MM-DD-YYYY-MM-DD format)\n",
    "            first_date_str = f\"{dates[0]}-{dates[1]}-{dates[2]}\"\n",
    "            second_date_str = f\"{dates[3]}-{dates[4]}-{dates[5]}\"\n",
    "            \n",
    "            # Convert to datetime objects\n",
    "            first_date = datetime.strptime(first_date_str, \"%Y-%m-%d\")\n",
    "            last_date = datetime.strptime(second_date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            posts.append({\n",
    "                \"file\": file,\n",
    "                \"first_date\": first_date,\n",
    "                \"last_date\": last_date\n",
    "            })\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Skipping file with invalid format: {file}. Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not posts:\n",
    "        return {\"oldest\": None, \"newest\": None}\n",
    "    \n",
    "    # Find the file with the earliest first date and the latest last date\n",
    "    oldest_post = min(posts, key=lambda x: x[\"first_date\"])\n",
    "    newest_post = max(posts, key=lambda x: x[\"last_date\"])\n",
    "    \n",
    "    return {\n",
    "        \"oldest\": {\n",
    "            \"file\": oldest_post[\"file\"],\n",
    "            \"first_date\": oldest_post[\"first_date\"].strftime(\"%Y-%m-%d\"),\n",
    "        },\n",
    "        \"newest\": {\n",
    "            \"file\": newest_post[\"file\"],\n",
    "            \"last_date\": newest_post[\"last_date\"].strftime(\"%Y-%m-%d\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def normalize_date(date_str, oldest_date_str, newest_date_str):\n",
    "    \"\"\"\n",
    "    Normalize a date to a value between -1 and 1, where:\n",
    "    - oldest_date maps to -1\n",
    "    - newest_date maps to 1\n",
    "    \n",
    "    Parameters:\n",
    "    date_str (str): Date to normalize in YYYY-MM-DD format\n",
    "    oldest_date_str (str): Reference start date in YYYY-MM-DD format\n",
    "    newest_date_str (str): Reference end date in YYYY-MM-DD format\n",
    "    \n",
    "    Returns:\n",
    "    float: Normalized value between -1 and 1\n",
    "    \"\"\"\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    oldest_date = datetime.strptime(oldest_date_str, \"%Y-%m-%d\")\n",
    "    newest_date = datetime.strptime(newest_date_str, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Calculate the total range in days\n",
    "    total_range = (newest_date - oldest_date).days\n",
    "    if total_range == 0:\n",
    "        return 0  # If oldest and newest are the same date\n",
    "    \n",
    "    # Calculate where our date falls within that range\n",
    "    days_from_start = (date - oldest_date).days\n",
    "    \n",
    "    # Normalize to [-1, 1] range\n",
    "    normalized_value = (2 * days_from_start / total_range) - 1\n",
    "    \n",
    "    return normalized_value\n",
    "\n",
    "def get_normalized_date_range(folder_path):\n",
    "    \"\"\"\n",
    "    Get the date range and provide a normalization function for dates within that range.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the files\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (date_range_dict, normalizer_function)\n",
    "    \"\"\"\n",
    "    date_range = get_oldest_and_newest_posts(folder_path)\n",
    "    \n",
    "    if date_range[\"oldest\"] is None or date_range[\"newest\"] is None:\n",
    "        return date_range, None\n",
    "    \n",
    "    oldest_date = date_range[\"oldest\"][\"first_date\"]\n",
    "    newest_date = date_range[\"newest\"][\"last_date\"]\n",
    "    \n",
    "    # Create a partial function with the date range pre-set\n",
    "    from functools import partial\n",
    "    normalizer = partial(normalize_date, \n",
    "                        oldest_date_str=oldest_date,\n",
    "                        newest_date_str=newest_date)\n",
    "    \n",
    "    return date_range, normalizer\n",
    "\n",
    "def analyze_feature_impact_across_files(folder_path):\n",
    "    \"\"\"\n",
    "    Analyzes the impact of features on normalized price across multiple files,\n",
    "    processing one file at a time to conserve memory. Uses pre-calculated\n",
    "    state price factors from state_price_factors.csv.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to folder containing the vehicle data CSV files\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays correlation plot and prints correlation values\n",
    "    \"\"\"\n",
    "    # Load pre-calculated state price factors\n",
    "    state_factors = pd.read_csv(os.path.join(folder_path, 'state_price_factors.csv'))\n",
    "    \n",
    "    # Columns to exclude from analysis\n",
    "    exclude_columns = [\n",
    "        'id', 'url', 'region', 'region_url', 'price', 'lat', 'long', \n",
    "        'posting_date', 'image_url', 'county', 'state', 'normalized_price',\n",
    "        'posting_year', 'car_age', 'state_price_factor'\n",
    "    ]\n",
    "    \n",
    "    # Initialize correlation sums and counts\n",
    "    correlation_sums = {}\n",
    "    correlation_counts = {}\n",
    "    \n",
    "    # Get list of cleaned vehicle files\n",
    "    files = glob(os.path.join(folder_path, \"cleaned-vehicles-*.csv\"))\n",
    "    \n",
    "    # First pass: identify all possible features\n",
    "    first_file = True\n",
    "    for file in files:\n",
    "        print(f\"Processing {os.path.basename(file)}...\")\n",
    "        \n",
    "        # Read file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        if first_file:\n",
    "            # Get relevant columns from first file\n",
    "            relevant_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "            for col in relevant_columns:\n",
    "                correlation_sums[col] = 0\n",
    "                correlation_counts[col] = 0\n",
    "            first_file = False\n",
    "        \n",
    "        # Add state price factors and normalized prices\n",
    "        df['state_price_factor'] = df['state'].map(\n",
    "            state_factors.set_index('state')['price_factor'].fillna(0)\n",
    "        )\n",
    "        df['normalized_price'] = df.apply(\n",
    "            lambda row: row['price'] / (1 + row['state_price_factor']/100), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Process each relevant column\n",
    "        for col in relevant_columns:\n",
    "            if col in df.columns:  # Check if column exists in this file\n",
    "                # Create copy of relevant data\n",
    "                temp_df = df[[col, 'normalized_price']].copy()\n",
    "                \n",
    "                # Handle non-numeric columns\n",
    "                if temp_df[col].dtype == 'object':\n",
    "                    temp_df[col] = pd.factorize(temp_df[col])[0]\n",
    "                \n",
    "                # Remove rows with NaN values\n",
    "                temp_df = temp_df.dropna()\n",
    "                \n",
    "                if len(temp_df) > 0:  # Only calculate if we have data\n",
    "                    # Calculate correlation\n",
    "                    corr = temp_df[col].corr(temp_df['normalized_price'])\n",
    "                    if not np.isnan(corr):\n",
    "                        correlation_sums[col] += corr\n",
    "                        correlation_counts[col] += 1\n",
    "        \n",
    "        # Clear memory\n",
    "        del df\n",
    "    \n",
    "    # Calculate average correlations\n",
    "    avg_correlations = {\n",
    "        col: correlation_sums[col] / correlation_counts[col]\n",
    "        for col in correlation_sums\n",
    "        if correlation_counts[col] > 0\n",
    "    }\n",
    "    \n",
    "    # Convert to Series and sort\n",
    "    correlation_series = pd.Series(avg_correlations).sort_values(ascending=False)\n",
    "    \n",
    "    # Print correlations\n",
    "    print(\"\\nAverage feature correlations with normalized price:\")\n",
    "    print(correlation_series)\n",
    "    \n",
    "    # Plot correlation barplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=correlation_series.index, y=correlation_series.values)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(\"Average Feature Correlation with Normalized Price Across All Files\")\n",
    "    plt.ylabel(\"Average Correlation Coefficient\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class DataNormalizer:\n",
    "    def __init__(self):\n",
    "        # Ordinal mappings\n",
    "        self.size_order = {\n",
    "            'sub-compact': 0,\n",
    "            'compact': 1,\n",
    "            'mid-size': 2,\n",
    "            'full-size': 3\n",
    "        }\n",
    "        \n",
    "        self.cylinder_order = {\n",
    "            'other': 0,\n",
    "            '3 cylinders': 3,\n",
    "            '4 cylinders': 4,\n",
    "            '5 cylinders': 5,\n",
    "            '6 cylinders': 6,\n",
    "            '8 cylinders': 8,\n",
    "            '10 cylinders': 10,\n",
    "            '12 cylinders': 12\n",
    "        }\n",
    "        \n",
    "        self.condition_order = {\n",
    "            'salvage': 0,\n",
    "            'fair': 1,\n",
    "            'good': 2,\n",
    "            'excellent': 3,\n",
    "            'like new': 4,\n",
    "            'new': 5\n",
    "        }\n",
    "        \n",
    "        # Reverse mappings for denormalization\n",
    "        self.size_reverse = {v: k for k, v in self.size_order.items()}\n",
    "        self.cylinder_reverse = {v: k for k, v in self.cylinder_order.items()}\n",
    "        self.condition_reverse = {v: k for k, v in self.condition_order.items()}\n",
    "        \n",
    "        # Storage for normalization parameters\n",
    "        self.numeric_params = {}\n",
    "        self.categorical_mappings = {}\n",
    "        \n",
    "        # Storage for imputation mappings\n",
    "        self.model_cylinders_map = {}\n",
    "        self.manufacturer_cylinders_map = {}\n",
    "        self.manufacturer_model_map = {}\n",
    "        self.most_common_cylinders = None\n",
    "        self.most_common_model = None\n",
    "        \n",
    "        # Define column types\n",
    "        self.numeric_cols = ['price', 'year', 'odometer']\n",
    "        self.categorical_cols = [\n",
    "            'fuel', 'title_status', 'transmission', \n",
    "            'drive', 'type', 'state', 'county'\n",
    "        ]\n",
    "        self.encode_only_cols = ['paint_color', 'region', 'manufacturer', 'model']\n",
    "\n",
    "    def fit(self, file_paths):\n",
    "        \"\"\"\n",
    "        Calculate normalization parameters and imputation mappings from multiple files.\n",
    "        \n",
    "        Parameters:\n",
    "        file_paths (list): List of paths to CSV files\n",
    "        \"\"\"\n",
    "        print(\"Calculating normalization parameters and imputation mappings...\")\n",
    "        \n",
    "        # Initialize parameters for numeric columns\n",
    "        numeric_mins = {col: float('inf') for col in self.numeric_cols}\n",
    "        numeric_maxs = {col: float('-inf') for col in self.numeric_cols}\n",
    "        \n",
    "        # Initialize sets for categorical values\n",
    "        categorical_values = {col: set() for col in self.categorical_cols}\n",
    "        encode_only_values = {col: set() for col in self.encode_only_cols}\n",
    "        \n",
    "        # Initialize counters for imputation\n",
    "        model_cylinders_count = {}      # {model: {cylinders: count}}\n",
    "        manufacturer_cylinders_count = {}  # {manufacturer: {cylinders: count}}\n",
    "        manufacturer_model_count = {}    # {manufacturer: {model: count}}\n",
    "        cylinders_count = {}            # {cylinders: count}\n",
    "        model_count = {}                # {model: count}\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "            \n",
    "            # Read file in chunks to save memory\n",
    "            chunk_size = 10000\n",
    "            for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "                # Update numeric min/max\n",
    "                for col in self.numeric_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        chunk_min = chunk[col].dropna().min()\n",
    "                        chunk_max = chunk[col].dropna().max()\n",
    "                        if not pd.isna(chunk_min) and not pd.isna(chunk_max):\n",
    "                            numeric_mins[col] = min(numeric_mins[col], chunk_min)\n",
    "                            numeric_maxs[col] = max(numeric_maxs[col], chunk_max)\n",
    "                \n",
    "                # Update categorical values\n",
    "                for col in self.categorical_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        categorical_values[col].update(chunk[col].dropna().unique())\n",
    "                \n",
    "                # Update encode-only values\n",
    "                for col in self.encode_only_cols:\n",
    "                    if col in chunk.columns:\n",
    "                        encode_only_values[col].update(chunk[col].dropna().unique())\n",
    "                \n",
    "                # Count relationships for imputation\n",
    "                valid_rows = chunk[['manufacturer', 'model', 'cylinders']].dropna(subset=['manufacturer'])\n",
    "                \n",
    "                for _, row in valid_rows.iterrows():\n",
    "                    mfr = row['manufacturer']\n",
    "                    model = row['model']\n",
    "                    cyls = row['cylinders']\n",
    "                    \n",
    "                    # Count cylinders by model\n",
    "                    if pd.notna(model) and pd.notna(cyls):\n",
    "                        if model not in model_cylinders_count:\n",
    "                            model_cylinders_count[model] = {}\n",
    "                        model_cylinders_count[model][cyls] = model_cylinders_count[model].get(cyls, 0) + 1\n",
    "                    \n",
    "                    # Count cylinders by manufacturer\n",
    "                    if pd.notna(cyls):\n",
    "                        if mfr not in manufacturer_cylinders_count:\n",
    "                            manufacturer_cylinders_count[mfr] = {}\n",
    "                        manufacturer_cylinders_count[mfr][cyls] = manufacturer_cylinders_count[mfr].get(cyls, 0) + 1\n",
    "                        cylinders_count[cyls] = cylinders_count.get(cyls, 0) + 1\n",
    "                    \n",
    "                    # Count models by manufacturer\n",
    "                    if pd.notna(model):\n",
    "                        if mfr not in manufacturer_model_count:\n",
    "                            manufacturer_model_count[mfr] = {}\n",
    "                        manufacturer_model_count[mfr][model] = manufacturer_model_count[mfr].get(model, 0) + 1\n",
    "                        model_count[model] = model_count.get(model, 0) + 1\n",
    "        \n",
    "        # Store numeric parameters\n",
    "        for col in self.numeric_cols:\n",
    "            if numeric_mins[col] < numeric_maxs[col]:\n",
    "                self.numeric_params[col] = {\n",
    "                    'min': numeric_mins[col],\n",
    "                    'max': numeric_maxs[col]\n",
    "                }\n",
    "        \n",
    "        # Create categorical mappings\n",
    "        for col in self.categorical_cols:\n",
    "            if categorical_values[col]:\n",
    "                sorted_values = sorted(categorical_values[col])\n",
    "                mapping = {val: idx for idx, val in enumerate(sorted_values)}\n",
    "                reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "                self.categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': reverse_mapping,\n",
    "                    'max': len(mapping) - 1\n",
    "                }\n",
    "        \n",
    "        # Create encode-only mappings\n",
    "        for col in self.encode_only_cols:\n",
    "            if encode_only_values[col]:\n",
    "                sorted_values = sorted(encode_only_values[col])\n",
    "                mapping = {val: idx for idx, val in enumerate(sorted_values)}\n",
    "                reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "                self.categorical_mappings[col] = {\n",
    "                    'mapping': mapping,\n",
    "                    'reverse_mapping': reverse_mapping\n",
    "                }\n",
    "        \n",
    "        # Create imputation mappings\n",
    "        # Most common cylinders by model\n",
    "        for model, cyls_dict in model_cylinders_count.items():\n",
    "            self.model_cylinders_map[model] = max(cyls_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Most common cylinders by manufacturer\n",
    "        for mfr, cyls_dict in manufacturer_cylinders_count.items():\n",
    "            self.manufacturer_cylinders_map[mfr] = max(cyls_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Most common model by manufacturer\n",
    "        for mfr, model_dict in manufacturer_model_count.items():\n",
    "            self.manufacturer_model_map[mfr] = max(model_dict.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Global most common values\n",
    "        if cylinders_count:\n",
    "            self.most_common_cylinders = max(cylinders_count.items(), key=lambda x: x[1])[0]\n",
    "        if model_count:\n",
    "            self.most_common_model = max(model_count.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        print(\"Normalization parameters and imputation mappings calculated.\")\n",
    "        \n",
    "        # Print encoding ranges for encode-only columns\n",
    "        print(\"\\nEncoding ranges:\")\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in self.categorical_mappings:\n",
    "                n_values = len(self.categorical_mappings[col]['mapping'])\n",
    "                print(f\"{col}: 0 to {n_values - 1} ({n_values} unique values)\")\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\"\n",
    "        Normalize the DataFrame using stored parameters, with smart imputation for missing values.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Input DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Normalized DataFrame with imputed missing values\n",
    "        \"\"\"\n",
    "        if not self.numeric_params and not self.categorical_mappings:\n",
    "            raise ValueError(\"Normalizer has not been fitted. Call fit() first.\")\n",
    "            \n",
    "        df_norm = df.copy()\n",
    "        \n",
    "        # Impute missing cylinders\n",
    "        mask = df_norm['cylinders'].isna()\n",
    "        if mask.any():\n",
    "            for idx in df_norm[mask].index:\n",
    "                model = df_norm.loc[idx, 'model']\n",
    "                manufacturer = df_norm.loc[idx, 'manufacturer']\n",
    "                \n",
    "                # Try to fill based on model\n",
    "                if pd.notna(model) and model in self.model_cylinders_map:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.model_cylinders_map[model]\n",
    "                # Try to fill based on manufacturer\n",
    "                elif pd.notna(manufacturer) and manufacturer in self.manufacturer_cylinders_map:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.manufacturer_cylinders_map[manufacturer]\n",
    "                # Use global most common\n",
    "                elif self.most_common_cylinders is not None:\n",
    "                    df_norm.loc[idx, 'cylinders'] = self.most_common_cylinders\n",
    "        \n",
    "        # Impute missing models\n",
    "        mask = df_norm['model'].isna()\n",
    "        if mask.any():\n",
    "            for idx in df_norm[mask].index:\n",
    "                manufacturer = df_norm.loc[idx, 'manufacturer']\n",
    "                \n",
    "                # Try to fill based on manufacturer\n",
    "                if pd.notna(manufacturer) and manufacturer in self.manufacturer_model_map:\n",
    "                    df_norm.loc[idx, 'model'] = self.manufacturer_model_map[manufacturer]\n",
    "                # Use global most common\n",
    "                elif self.most_common_model is not None:\n",
    "                    df_norm.loc[idx, 'model'] = self.most_common_model\n",
    "        \n",
    "        # Normalize numeric columns\n",
    "        for col in self.numeric_cols:\n",
    "            if col in df_norm.columns and col in self.numeric_params:\n",
    "                params = self.numeric_params[col]\n",
    "                df_norm[col] = (df_norm[col] - params['min']) / (params['max'] - params['min'])\n",
    "        \n",
    "        # Normalize ordinal columns\n",
    "        # Size\n",
    "        if 'size' in df_norm.columns:\n",
    "            df_norm['size'] = df_norm['size'].map(self.size_order)\n",
    "            size_max = max(self.size_order.values())\n",
    "            df_norm['size'] = df_norm['size'] / size_max\n",
    "        \n",
    "        # Cylinders\n",
    "        if 'cylinders' in df_norm.columns:\n",
    "            df_norm['cylinders'] = df_norm['cylinders'].map(self.cylinder_order)\n",
    "            cylinder_max = max(self.cylinder_order.values())\n",
    "            df_norm['cylinders'] = df_norm['cylinders'] / cylinder_max\n",
    "        \n",
    "        # Condition\n",
    "        if 'condition' in df_norm.columns:\n",
    "            df_norm['condition'] = df_norm['condition'].map(self.condition_order)\n",
    "            condition_max = max(self.condition_order.values())\n",
    "            df_norm['condition'] = df_norm['condition'] / condition_max\n",
    "        \n",
    "        # Normalize categorical columns to 0-1\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_norm.columns and col in self.categorical_mappings:\n",
    "                mapping = self.categorical_mappings[col]['mapping']\n",
    "                max_val = self.categorical_mappings[col]['max']\n",
    "                df_norm[col] = df_norm[col].map(mapping) / max_val\n",
    "        \n",
    "        # Handle encode-only columns (simple integer encoding)\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in df_norm.columns and col in self.categorical_mappings:\n",
    "                mapping = self.categorical_mappings[col]['mapping']\n",
    "                df_norm[col] = df_norm[col].map(mapping)\n",
    "        \n",
    "        return df_norm\n",
    "\n",
    "    def denormalize(self, df_norm):\n",
    "        \"\"\"\n",
    "        Denormalize the DataFrame using stored parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        df_norm (pandas.DataFrame): Normalized DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Denormalized DataFrame with original scale and categories\n",
    "        \"\"\"\n",
    "        if not self.numeric_params and not self.categorical_mappings:\n",
    "            raise ValueError(\"Normalizer has not been fitted. Call fit() first.\")\n",
    "            \n",
    "        df_denorm = df_norm.copy()\n",
    "        \n",
    "        # Denormalize numeric columns\n",
    "        for col in self.numeric_cols:\n",
    "            if col in df_denorm.columns and col in self.numeric_params:\n",
    "                params = self.numeric_params[col]\n",
    "                df_denorm[col] = df_denorm[col] * (params['max'] - params['min']) + params['min']\n",
    "        \n",
    "        # Denormalize ordinal columns\n",
    "        # Size\n",
    "        if 'size' in df_denorm.columns:\n",
    "            size_max = max(self.size_order.values())\n",
    "            df_denorm['size'] = (df_denorm['size'] * size_max).round()\n",
    "            df_denorm['size'] = df_denorm['size'].map(self.size_reverse)\n",
    "        \n",
    "        # Cylinders\n",
    "        if 'cylinders' in df_denorm.columns:\n",
    "            cylinder_max = max(self.cylinder_order.values())\n",
    "            df_denorm['cylinders'] = (df_denorm['cylinders'] * cylinder_max).round()\n",
    "            df_denorm['cylinders'] = df_denorm['cylinders'].map(self.cylinder_reverse)\n",
    "        \n",
    "        # Condition\n",
    "        if 'condition' in df_denorm.columns:\n",
    "            condition_max = max(self.condition_order.values())\n",
    "            df_denorm['condition'] = (df_denorm['condition'] * condition_max).round()\n",
    "            df_denorm['condition'] = df_denorm['condition'].map(self.condition_reverse)\n",
    "        \n",
    "        # Denormalize categorical columns\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_denorm.columns and col in self.categorical_mappings:\n",
    "                max_val = self.categorical_mappings[col]['max']\n",
    "                reverse_mapping = self.categorical_mappings[col]['reverse_mapping']\n",
    "                df_denorm[col] = (df_denorm[col] * max_val).round()\n",
    "                df_denorm[col] = df_denorm[col].map(reverse_mapping)\n",
    "        \n",
    "        # Handle encode-only columns\n",
    "        for col in self.encode_only_cols:\n",
    "            if col in df_denorm.columns and col in self.categorical_mappings:\n",
    "                reverse_mapping = self.categorical_mappings[col]['reverse_mapping']\n",
    "                df_denorm[col] = df_denorm[col].round()\n",
    "                df_denorm[col] = df_denorm[col].map(reverse_mapping)\n",
    "        \n",
    "        return df_denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bcda10-bbf6-4081-b3a2-c1df76906297",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_feature_impact_across_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e1815-d3f0-44b3-b73d-b1c287e2b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned-vehicles-2021-04-04-2021-05-04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e631f-641e-4645-a7cb-953e753fb92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalizer = DataNormalizer()\n",
    "\n",
    "# Get all CSV files in the data directory\n",
    "file_paths = glob(\"data/cleaned-vehicles-*.csv\")\n",
    "\n",
    "# Fit the normalizer with all files\n",
    "normalizer.fit(file_paths)\n",
    "\n",
    "# Now you can normalize any individual file or DataFrame\n",
    "df = pd.read_csv(file_paths[0])\n",
    "normalized_df = normalizer.normalize(df)\n",
    "\n",
    "# And denormalize when needed\n",
    "original_scale_df = normalizer.denormalize(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a3ef4-9f65-46d4-aac7-d322f3b6f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a036f0-34be-40eb-8f4d-a774221cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paint_color\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca340e-fd4b-4c2f-b3a9-d16b48c2df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
